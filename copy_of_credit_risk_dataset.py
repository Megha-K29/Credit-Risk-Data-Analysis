# -*- coding: utf-8 -*-
"""Copy of Credit_Risk_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12wuXXKGpfko7CcWmCLON1ZG8zDKuuaQx
"""

# CREDIT RISK DATA ANALYSIS

# Feature Descriptions
# 	• person_age: Age of the individual applying for the loan.
# 	• person_income: Annual income of the individual.
# 	• person_home_ownership: Type of home ownership of the individual.
# 		○ rent: The individual is currently renting a property.
# 		○ mortgage: The individual has a mortgage on the property they own.
# 		○ own: The individual owns their home outright.
# 		○ other: Other categories of home ownership that may be specific to the dataset.
# 	• person_emp_length: Employment length of the individual in years.
# 	• loan_intent: The intent behind the loan application.
# 	• loan_grade: The grade assigned to the loan based on the creditworthiness of the borrower.
# 		○ A: The borrower has a high creditworthiness, indicating low risk.
# 		○ B: The borrower is relatively low-risk, but not as creditworthy as Grade A.
# 		○ C: The borrower's creditworthiness is moderate.
# 		○ D: The borrower is considered to have higher risk compared to previous grades.
# 		○ E: The borrower's creditworthiness is lower, indicating a higher risk.
# 		○ F: The borrower poses a significant credit risk.
# 		○ G: The borrower's creditworthiness is the lowest, signifying the highest risk.
# 	• loan_amnt: The loan amount requested by the individual.
# 	• loan_int_rate: The interest rate associated with the loan.
# 	• loan_status: Loan status, where 0 indicates non-default and 1 indicates default.
# 		○ 0: Non-default - The borrower successfully repaid the loan as agreed, and there was no default.
# 		○ 1: Default - The borrower failed to repay the loan according to the agreed-upon terms and defaulted on the loan.
# 	• loan_percent_income: The percentage of income represented by the loan amount.
# 	• cb_person_default_on_file: Historical default of the individual as per credit bureau records.
# 		○ Y: The individual has a history of defaults on their credit file.
# 		○ N: The individual does not have any history of defaults.
# 	• cb_preson_cred_hist_length: The length of credit history for the individual.

"""I'm working on a project with a dataset that's all about understanding how likely people are to pay back loans. This is important because it affects both the people borrowing money and the companies lending it.
The dataset has information like how old people are, how much money they make, whether they own a home, how long they've been working, why they want a loan, and so on.
Now, why am I doing this? Well, there are a few reasons:
 Helping Lenders: I want to create a tool that helps companies decide if someone is a good or bad bet for a loan. If they're a good bet, it's more likely they'll get the loan they need.


 Protecting People: I also want to make sure that people don't end up with loans they can't handle. It's about keeping folks from getting into financial trouble.

 Business Success: When companies make smart choices about who they lend money to, they tend to do better financially.

 Economic Stability: Lastly, when lots of people can't pay back their loans, it can mess up the whole economy. So, by doing this project, I hope to help prevent big financial problems.

In a nutshell, this project isn't just about numbers; it's about making sure loans are fair and safe for everyone involved, from borrowers to lenders and even the whole economy.

#Importing necessary libraries and reading the data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Load the credit risk dataset from the CSV file into a Pandas DataFrame
df=pd.read_csv('/content/credit_risk_dataset.csv')
df

"""#Printing first few rows"""

df.head()

"""#Printing last few rows"""

df.tail()

"""#Checking column data types"""

df.dtypes

"""#Information about dataframe"""

df.info()

"""Helps to get the Statistical Summary
    Count: This row shows the number of non-null (non-missing) values in each numeric column. It helps you identify missing data or columns with fewer observations.

    Mean: The mean (average) of the data in each column is shown in this row. It represents the central tendency of the data.

    Std: The standard deviation measures the spread or variability of the data. It indicates how much individual data points deviate from the mean.

    Min: This row displays the minimum value in each column, which is the smallest observed value.

    25%: The 25th percentile (first quartile) represents the value below which 25% of the data falls. It provides information about the lower range of the data.

    50% (median): The median is the middle value of the data when sorted. It is a measure of central tendency that is less affected by outliers compared to the mean.

    75%: The 75th percentile (third quartile) represents the value below which 75% of the data falls. It provides information about the upper range of the data.

    Max: This row displays the maximum value in each column, which is the largest observed value.
"""

df.describe()

df.describe(include='object')
# The 'count' column represents the number of non-null entries in each categorical column.
# The 'unique' column indicates the number of distinct categories in each column.
# The 'top' column shows the most frequent category in each column.
# The 'freq' column displays the frequency of the most frequent category.

"""Correlation Matrix: A correlation matrix is a table that shows the correlation coefficients between many variables. In your case, it calculates the correlations between all pairs of numerical columns in your DataFrame."""

# Calculate the correlation matrix for the entire DataFrame
df.corr()

"""#Finding out the missing values"""

df.isna().sum()

"""#Counting distinct values per column"""

df.nunique()

"""#Checking duplicated rows in dataframe"""

dup=df.duplicated()
df[dup]

# Query the DataFrame to filter rows with specific conditions
df.query("person_age==23 & person_income==42000 & loan_grade=='B' & loan_amnt==6000")

# Get the dimensions (rows and columns) of the DataFrame
df.shape

# Remove duplicate rows in the DataFrame
df.drop_duplicates(inplace=True)

df.shape

"""#Create a heatmap to visualize the correlation matrix

 A heatmap is like a colorful map for your data. Each square in the map represents how two things in your data are related. In our case, it shows how different columns (or features) in our dataset are related to each other.he colors in the heatmap indicate the strength of the relationship. Dark green means a strong positive relationship (when one goes up, the other goes up)
 Annotations: The numbers inside the squares are just to help us understand the exact strength of the relationship. The higher the number, the stronger the relationship.
"""

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(),annot=True,cmap='Greens',linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""This information is crucial because it helps us understand the balance or distribution of loan statuses. For example, if most loans are approved, it might indicate a lenient approval process, while a higher percentage of denied loans could imply a stricter process."""

# Calculate and display the normalized value counts of the 'loan_status' column
df['loan_status'].value_counts(normalize=True)

sns.countplot(x='loan_status',data=df,palette='summer')

plt.figure(figsize=(15,11))
plt.subplot(2,3,1)
sns.histplot(x='person_age',data=df,kde=True,bins=20)
plt.title('Person Age')

plt.subplot(2,3,2)
sns.histplot(x='person_income',data=df,kde=True,bins=20)
plt.title('Person Income')

plt.subplot(2,3,3)
sns.histplot(x='person_emp_length',data=df,kde=True,bins=20)
plt.title('Person Employment Length')

plt.subplot(2,3,4)
sns.histplot(x='loan_amnt',data=df,kde=True,bins=20)
plt.title('Loan Amount')

plt.subplot(2,3,5)
sns.histplot(x='loan_int_rate',data=df,kde=True,bins=20)
plt.title('Loan Interest Rate')

plt.subplot(2,3,6)
sns.histplot(x='cb_person_cred_hist_length',data=df,kde=True,bins=20)
plt.title('Credit History Length')

plt.tight_layout()
plt.show()

"""Pie Chart:I am creating a pie chart to visually represent this distribution. In a pie chart, each category is shown as a slice of the pie, with the size of the slice representing the proportion or percentage of that category in the dataset.

    plt.legend(): This function adds a legend to your plot, and the bbox_to_anchor parameter specifies the exact location of the legend relative to the plot's bounding box.


"""

home_ownership_counts=df['person_home_ownership'].value_counts()
plt.figure(figsize=(6,6))
plt.pie(home_ownership_counts,labels=home_ownership_counts.index,autopct='%1.1f%%',
        colors=sns.color_palette('Accent'),startangle=180,shadow=True)
plt.title("Home Ownership Distribution")
plt.legend(loc='upper left',bbox_to_anchor=(1,0.5))
plt.show()
home_ownership_counts

loan_intent_count=df['loan_intent'].value_counts()
plt.figure(figsize=(6,6))
plt.pie(loan_intent_count,labels=loan_intent_count.index,colors=sns.color_palette('Accent'),
        autopct='%1.1f%%',startangle=140,shadow=True)
plt.title("Home Ownership Distribution")
plt.legend(title='Home Ownership',loc='upper left',bbox_to_anchor=(1,0.5))
plt.show()
loan_intent_count

loan_grade_count=df['loan_grade'].value_counts()
loan_grade_count

loan_grade_count=df['loan_grade'].value_counts()
loan_grade_count
plt.figure(figsize=(8,6))
sns.countplot(x='loan_grade',data=df,palette='viridis')
plt.title('Count of Loan Grade')
plt.xlabel('Loan Grade')
plt.ylabel('Count')

default_on_file_count=df['cb_person_default_on_file'].value_counts()
plt.figure(figsize=(6,6))
plt.pie(default_on_file_count,labels=default_on_file_count.index,autopct='%1.1f%%',
        colors=sns.color_palette('Set2'),shadow=True)
plt.title('Default on file distribution')
plt.legend(title='Distribution',loc='upper left',bbox_to_anchor=(1,0.5))
default_on_file_count

#Analysing categorical variable with target variable
fig,axes=plt.subplots(2,2,figsize=(21,10))
sns.countplot(x='person_home_ownership',hue='loan_status',data=df,ax=axes[0,0],palette='Reds_r')
sns.countplot(x='loan_intent',hue='loan_status',data=df,ax=axes[0,1],palette='Reds_r')
sns.countplot(x='loan_grade',hue='loan_status',data=df,ax=axes[1,1],palette='Reds_r')
sns.countplot(x='cb_person_default_on_file',hue='loan_status',data=df,ax=axes[1,0],palette='Reds_r')
plt.show()

"""This code generates boxplots for the selected features ('person_age,' 'person_income,' 'person_emp_length,' 'loan_percent_income,' 'cb_person_cred_hist_length') to visualize and identify any potential outliers. Boxplots provide information about the distribution of data, highlighting outliers and showing the median, quartiles, and other statistics. The 'showmeans=True' parameter displays the mean value on the boxplots."""

features=['person_age','person_income','person_emp_length','loan_percent_income','cb_person_cred_hist_length']
plt.figure(figsize=(10,5))
for i in range(0,len(features)):
  plt.subplot(1,len(features),i+1)
  sns.boxplot(y=df[features[i]],palette='Purples',orient='v',showmeans=True)
  plt.tight_layout()

#Looking for outliers
df['person_age'].sort_values(ascending=False).head(10)

df['person_emp_length'].sort_values(ascending=False).head(10)

# Remove rows where 'person_age' is greater than 80
df.drop(df.loc[df['person_age']>80].index,inplace=True)

df.loc[df['person_age']>80]

# Remove rows where 'person_emp_length' is equal to 123
df.drop(df.loc[df['person_emp_length']==123].index,inplace=True)

df.loc[df['person_emp_length']==123]

#Verifying outliers are present or not
fig,axes=plt.subplots(1,2,figsize=(12,6))
sns.boxplot(y='person_age',data=df,ax=axes[0],palette='cubehelix')
sns.boxplot(y='person_emp_length',data=df,ax=axes[1],palette='Accent')

# Create a pair plot with hue for 'loan_status'
sns.pairplot(df,hue='loan_status')

# Create a bar plot to show age by home ownership
plt.figure(figsize=(10,5))

plt.title("Age by Ownership")

sns.barplot(x=df["person_home_ownership"], y=df["person_age"], palette="RdPu")

plt.figure(figsize=(10,5))

plt.title('Relation between income and loan grade')

sns.barplot(x=df['loan_grade'], y=df['person_income'],palette='YlOrBr')

plt.figure(figsize=(10,5))

plt.title('Relation between income and person default')

sns.barplot(x=df['cb_person_default_on_file'], y=df['person_income'],palette='YlGn')

# Reset the DataFrame index
df.reset_index(inplace=True)

# Calculate the median of 'person_emp_length' and 'loan_int_rate' columns
x=df['person_emp_length'].median()
df['person_emp_length'].fillna(x,inplace=True)
y=df['loan_int_rate'].median()
df['loan_int_rate'].fillna(y,inplace=True) # Fill missing values in 'person_emp_length' and 'loan_int_rate' with their respective medians
# Print the medians
print('Median of Person Employment Length is',x)
print('Median of Loan Interest Rate is',y)

df.isna().sum()

df.dtypes

"""Encoding in the context of data and machine learning refers to the process of converting data from one representation or format into another, often to make it suitable for analysis or to be used as input for machine learning algorithms.
The get_dummies function in pandas is a method for one-hot encoding categorical variables. One-hot encoding is a technique used to convert categorical (non-numeric) data into a numeric format, making it suitable for machine learning algorithms.
"""

# Create dummy variables for categorical columns with drop_first option
data_encoded=pd.get_dummies(df[['person_home_ownership','loan_intent','loan_grade','cb_person_default_on_file']],drop_first=True)
data_encoded

# Concatenate the original DataFrame and the one-hot encoded data
df1=pd.concat([df,data_encoded],axis=1)
df1

df1.drop(['person_home_ownership','loan_intent','loan_grade','cb_person_default_on_file'],axis=1,inplace=True)
df1

df1.head()

df1.describe()

df1.columns

df1.dtypes

x=df1.drop(['loan_status'],axis=1)
x

y=df1['loan_status']
y

#Balancing an imbalanced data set
#oversampling using BorderLineSMOTE(BorderLine Synthetic Minority Oversampling Techniques)


from imblearn.over_sampling import BorderlineSMOTE

# Assuming 'y' is your target variable
# Check for NaN values and handle them, e.g., by removing rows with NaN values
y = y.dropna()

# Continue with the oversampling process
os = BorderlineSMOTE()
x_os, y_os = os.fit_resample(x, y)

y.value_counts()

y_os.value_counts()

#Splitting
from sklearn.model_selection import train_test_split
x_train_os,x_test_os,y_train_os,y_test_os=train_test_split(x_os,y_os,test_size=0.30,random_state=42)

print(x_train_os.shape,x_test_os.shape)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit_transform(x_train_os)
x_train_os=scaler.transform(x_train_os)
x_test_os=scaler.transform(x_test_os)

x_train_os

x_test_os

"""    K-Nearest Neighbors (KNN):
        K-Nearest Neighbors is a supervised machine learning algorithm used for classification and regression tasks.
        It classifies data points based on their similarity to other data points.
        It makes predictions by finding the majority class among the k-nearest data points.

    Naive Bayes (BernoulliNB):
        Naive Bayes is a family of probabilistic algorithms that use the Bayes' theorem to predict the probability of a certain event, given a set of features.
        Bernoulli Naive Bayes is specifically designed for binary or Boolean features.
        It's commonly used for text classification and spam detection.

    Support Vector Machine (SVM):
        Support Vector Machine is a supervised learning algorithm used for classification and regression.
        It finds a hyperplane that best separates data points into different classes.
        SVM aims to maximize the margin between classes, making it robust to overfitting.

    Logistic Regression:
        Logistic Regression is a statistical method used for binary classification.
        Despite its name, it's a classification algorithm, not regression.
        It models the probability that a data point belongs to a particular class based on the values of its features.

    Decision Tree:
        Decision Trees are a versatile algorithm used for classification and regression tasks.
        They partition the data into subsets based on feature values and create a tree structure of decisions.
        They are known for their interpretability and ability to handle non-linear relationships.

    Random Forest:
        Random Forest is an ensemble learning method that combines multiple decision trees to improve predictive accuracy.
        It reduces overfitting and increases the robustness of the model.
        Each tree is trained on a random subset of the data, and their predictions are aggregated.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
knn=KNeighborsClassifier(n_neighbors=7)
nb=BernoulliNB()
svc=SVC()
log_reg=LogisticRegression()
tree=DecisionTreeClassifier()
forest=RandomForestClassifier()
lst_model=[knn,nb,svc,log_reg,tree,forest]

from sklearn.metrics import confusion_matrix,accuracy_score,ConfusionMatrixDisplay,classification_report

for i in lst_model:
    print(i)
    i.fit(x_train_os, y_train_os)
    y_pred = i.predict(x_test_os)
    print("*******************************************")
    print(classification_report(y_test_os, y_pred))
    print("*******************************************")
    print(confusion_matrix(y_test_os, y_pred))
    labels=[0,1]
    result=confusion_matrix(y_test_os,y_pred)
    cmd=ConfusionMatrixDisplay(result,display_labels=labels)
    cmd.plot()

accuracy_scores = []
model_names = ['K Nearest Neighbor', 'Naive Bayes', 'SVM', 'Logistic', 'Decision Tree', 'Random Forest']
for model in lst_model:
    model.fit(x_train_os, y_train_os)
    y_pred=model.predict(x_test_os)
    accuracy = accuracy_score(y_test_os, y_pred)
    accuracy_scores.append(accuracy)

comparison = pd.DataFrame({'Model': model_names, 'Accuracy': accuracy_scores})
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Accuracy', data=comparison, palette='Blues')
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores Comparison for Different Models')
for i, score in enumerate(accuracy_scores):
    plt.text(i, score + 0.01, f'{score:.2f}', ha='center')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""KNN performs well in terms of overall accuracy, with a balanced precision and recall for both classes.

BernoulliNB provides reasonable accuracy and balanced precision-recall scores for both classes.

SVC demonstrates a high level of accuracy with strong precision and recall for both classes.

Logistic Regression provides a balanced approach with reasonable accuracy, precision, and recall for both classes.

The Decision Tree Classifier shows good accuracy and balanced precision-recall scores for both classes.

Random Forest Classifier exhibits high accuracy and strong precision-recall scores, especially for class 0.




Conclusion:

    The Random Forest Classifier appears to be the best-performing model in terms of overall accuracy, precision, recall, and F1-Score for both classes.
"""